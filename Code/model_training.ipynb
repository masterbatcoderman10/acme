{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import graphviz\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Data/ACME-HappinessSurvey2020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"Y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns  = list(data.columns)\n",
    "columns.remove(\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = data[[\"X1\", \"X6\"]]\n",
    "X = data[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    \"order delivered on time\",\n",
    "    \"contents were as expected\",\n",
    "    \"I ordered everything I wanted to order\",\n",
    "    \"I paid a good price\",\n",
    "    \"I am satisfied with the courier\",\n",
    "    \"the app is easy to order\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [0, 5]\n",
    "\n",
    "selected_features = [feature_names[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6587301587301587"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_classifier = DecisionTreeClassifier(criterion=\"gini\")\n",
    "val_scores_dt = cross_val_score(dt_classifier, X, y, scoring=\"accuracy\", cv=6)\n",
    "np.mean(val_scores_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6507936507936508\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = DecisionTreeClassifier(criterion=\"entropy\", min_samples_leaf=16)\n",
    "dt_classifier = dt_classifier.fit(X, y)\n",
    "preds = dt_classifier.predict(X)\n",
    "acc_score = accuracy_score(y, preds)\n",
    "print(acc_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6428571428571428"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.1, min_samples_leaf=12,random_state=5)\n",
    "val_scores_gbc = cross_val_score(gbc, X, y, scoring=\"accuracy\", cv=6)\n",
    "np.mean(val_scores_gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4230769230769231\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.1, min_samples_leaf=12,random_state=5)\n",
    "gbc = gbc.fit(X_train, y_train)\n",
    "preds = gbc.predict(X_test)\n",
    "acc_score = accuracy_score(y_test, preds)\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9365079365079365\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.1, min_samples_leaf=12,random_state=5)\n",
    "gbc = gbc.fit(X, y)\n",
    "train_preds = gbc.predict(X)\n",
    "acc_score = accuracy_score(y, train_preds)\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gbc.joblib']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "dump(gbc, \"gbc.joblib\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5396825396825397"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=3)\n",
    "val_scores_rf = cross_val_score(rf, X, y, scoring=\"accuracy\", cv=3)\n",
    "np.mean(val_scores_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=3)\n",
    "rf = rf.fit(X, y)\n",
    "preds = rf.predict(X)\n",
    "acc_score = accuracy_score(y, preds)\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46153846153846156\n"
     ]
    }
   ],
   "source": [
    "dt_classifier = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "dt_classifier = dt_classifier.fit(X_train, y_train)\n",
    "y_preds = dt_classifier.predict(X_test)\n",
    "acc_score = accuracy_score(y_test, y_preds)\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(\n",
    "    dt_classifier,\n",
    "    feature_names=selected_features,\n",
    "    class_names=[\"unhappy\", \"happy\"],\n",
    "    filled=True\n",
    ")\n",
    "\n",
    "plt.savefig(\"dt.png\", dpi=200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5317460317460317"
      ]
     },
     "execution_count": 1219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model = SVC(kernel=\"poly\")\n",
    "val_scores_svm = cross_val_score(svm_model, X, y, scoring=\"accuracy\", cv=6)\n",
    "np.mean(val_scores_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(kernel=\"poly\")\n",
    "svm_model = svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = accuracy_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5789473684210527"
      ]
     },
     "execution_count": 965,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6031746031746031"
      ]
     },
     "execution_count": 915,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "val_scores_lr = cross_val_score(lr, X, y, scoring=\"accuracy\", cv=6)\n",
    "np.mean(val_scores_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.611111111111111"
      ]
     },
     "execution_count": 916,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "val_scores_nb = cross_val_score(nb, X, y, scoring=\"accuracy\",cv=6)\n",
    "np.mean(val_scores_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg:\n",
    "\n",
    "    def __init__(self, y, X, eta, epochs, momentum=False, alpha=0.0, irls=False):\n",
    "        self.w = np.random.normal(0.0, 1.0, (X.shape[1],)) \n",
    "        self.y = y\n",
    "        self.X = X\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.momentum = momentum\n",
    "        #Iterative Re-weighted least squares\n",
    "        self.irls = irls\n",
    "        if momentum:\n",
    "            self.velocity = 0.0\n",
    "            #Parameter to update velocity\n",
    "            self.alpha = alpha\n",
    "\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-1 * x))\n",
    "    \n",
    "    \n",
    "    def predict(self, x):\n",
    "        preds = np.dot(self.w, x)\n",
    "        return preds\n",
    "    \n",
    "    def pred_prob(self, x):\n",
    "        return self.sigmoid(self.predict(x))\n",
    "\n",
    "    def full_prob(self, X):\n",
    "\n",
    "        return self.sigmoid(X @ self.w)\n",
    "    \n",
    "    def full_cost(self):\n",
    "\n",
    "        cost = 0\n",
    "        for x_i, y_i in zip(self.X, self.y):\n",
    "            cost += self.calc_cost(x_i, y_i)\n",
    "        \n",
    "        return cost / len(self.X)\n",
    "\n",
    "    \n",
    "    def calc_cost(self, x, y):\n",
    "\n",
    "        if y == 1:\n",
    "            return -1 * np.log(self.sigmoid(self.predict(x)))\n",
    "        else:\n",
    "            return -1 * np.log(1 - self.sigmoid(self.predict(x)))\n",
    "    \n",
    "    def full_grad(self, lambd=0.0):\n",
    "        \n",
    "        #lambda for regularization\n",
    "        P1 = self.full_prob(self.X)\n",
    "        return self.X.T @ (P1 - self.y) + (lambd * self.w)\n",
    "\n",
    "    def calc_hessian(self, lambd=0.0):\n",
    "\n",
    "        P1 = self.full_prob(self.X)\n",
    "        diagonals = P1 * (1 - P1)\n",
    "\n",
    "        n = len(P1)\n",
    "        R = np.zeros((n,n))\n",
    "        np.fill_diagonal(R, diagonals)\n",
    "        \n",
    "        H = (self.X.T @ R @ self.X) + lambd\n",
    "\n",
    "        return H\n",
    "    \n",
    "    def single_grad(self, x, y, l2=False, lamb=0.0):\n",
    "\n",
    "        if l2:\n",
    "            if y == 1:\n",
    "                return -1 * x * (1 - self.sigmoid(self.predict(x))) + lamb * self.w\n",
    "            else:\n",
    "                return x * self.sigmoid(self.predict(x)) + lamb * self.w\n",
    "\n",
    "\n",
    "        #Gives gradient for single input variable\n",
    "        if y == 1:\n",
    "            return -1 * x * (1 - self.sigmoid(self.predict(x)))\n",
    "        else:\n",
    "            return x * self.sigmoid(self.predict(x))\n",
    "    \n",
    "    def update_weights(self, grad=None, hessian=None):\n",
    "\n",
    "        if self.momentum:\n",
    "            self.w = self.w + self.velocity\n",
    "        elif self.irls:\n",
    "            self.w = self.w - (hessian * grad)\n",
    "        else:\n",
    "            self.w = self.w - (self.eta * grad)\n",
    "\n",
    "    \n",
    "    def shuffle_together(self, x, y):\n",
    "        assert len(x) == len(y)\n",
    "        p = np.random.permutation(len(x))\n",
    "        return x[p], y[p]\n",
    "    \n",
    "    def gen_batches(self, x, y, batch_size):\n",
    "\n",
    "        batches = []\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "\n",
    "        while end < len(x):\n",
    "            batches.append((x[start:end], y[start:end]))\n",
    "            start = end\n",
    "            end += batch_size\n",
    "\n",
    "        return np.array(batches)\n",
    "    \n",
    "    def train_gd_eff(self, lambd=0.0):\n",
    "\n",
    "        n = len(self.X)\n",
    "        cost = self.full_cost()\n",
    "        print(f\"The loss before training : {cost}\")\n",
    "        for e in range(self.epochs):\n",
    "            \n",
    "            grad = self.full_grad(lambd)\n",
    "            self.update_weights(grad)\n",
    "        \n",
    "        cost = self.full_cost()\n",
    "        print(f\"The loss after training : {cost}\")\n",
    "\n",
    "    def train_irls(self, lambd=0.0):\n",
    "\n",
    "        cost = self.full_cost()\n",
    "        print(f\"The loss before training : {cost}\")\n",
    "        for e in range(self.epochs):\n",
    "            \n",
    "            grad = self.full_grad()\n",
    "\n",
    "            #Calculate the Hessian matrix\n",
    "            hessian = self.calc_hessian(lambd)\n",
    "            #Invert the Hessian matrix\n",
    "            hessian = np.linalg.inv(hessian)\n",
    "            print(hessian.shape)\n",
    "            #Passing in the Hessian since learning rate won't be used\n",
    "            self.update_weights(grad, hessian)\n",
    "        \n",
    "        cost = self.full_cost()\n",
    "        print(f\"The loss after training : {cost}\")\n",
    "\n",
    "\n",
    "    \n",
    "    def train_sgd(self, l2=False, l=0):\n",
    "        n = len(self.X)\n",
    "        for e in range(self.epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(n):\n",
    "\n",
    "                i = np.random.randint(0, n)\n",
    "                x_i = self.X[i]\n",
    "                y_i = self.y[i]\n",
    "\n",
    "                loss = self.calc_cost(x_i, y_i)\n",
    "                grad = self.single_grad(x_i, y_i, l2, l)\n",
    "\n",
    "                epoch_loss += loss\n",
    "                # print(f\"The observation : {x_i}\")\n",
    "                # print(f\"The target : {y_i}\")\n",
    "                # print(f\"Previous weights : {self.w}\")\n",
    "                # print(f\"The dot product of weight with observation : {self.predict(x_i)}\")\n",
    "                # print(f\"Sigmoid of prediction : {self.pred_prob(x_i)}\")\n",
    "                self.update_weights(grad)\n",
    "                # print(f\"Gradient update at epoch {e} : {grad}\")\n",
    "            epoch_loss /= n\n",
    "            print(f\"Loss at epoch {e} : {epoch_loss}\")\n",
    "                # print(f\"Weight after update : {self.w}\")\n",
    "                # print(\"\\n\")\n",
    "    def calc_velocity(self, grad):\n",
    "        \n",
    "        self.velocity = (self.alpha * self.velocity) - (self.eta * grad)\n",
    "\n",
    "    def train_gd(self, l2=False, l=0):\n",
    "        n = len(self.X)\n",
    "        for e in range(self.epochs):\n",
    "            total_loss = 0\n",
    "            avg_grad = np.zeros(self.w.shape)\n",
    "            #print(f\"Previous weights at epoch {e} : {self.w}\")\n",
    "            for i in range(0, n):\n",
    "                x_i = self.X[i]\n",
    "                y_i = self.y[i]\n",
    "                total_loss += self.calc_cost(x_i, y_i)\n",
    "                avg_grad = avg_grad + self.single_grad(x_i, y_i, l2, l)\n",
    "\n",
    "            print(f\"Loss at epoch {e} : {total_loss / n}\")\n",
    "            print(f\"The gradient is : {avg_grad}\")\n",
    "            #print(f\"Gradients : {avg_grad}\")\n",
    "            #avg_grad = avg_grad / n\n",
    "\n",
    "            if self.momentum:\n",
    "                self.calc_velocity(avg_grad)\n",
    "                self.update_weights()\n",
    "            else:\n",
    "                self.update_weights(avg_grad)\n",
    "            #print(f\"Weight after update at epoch {e} : {self.w}\")\n",
    "            #print(\"\\n\")\n",
    "    def compare(self):\n",
    "        self.w = np.zeros(6,)\n",
    "        n = len(self.X)\n",
    "        for e in range(self.epochs):\n",
    "            total_loss = 0\n",
    "            avg_grad = np.zeros(self.w.shape)\n",
    "            #print(f\"Previous weights at epoch {e} : {self.w}\")\n",
    "            for i in range(0, n):\n",
    "                x_i = self.X[i]\n",
    "                y_i = self.y[i]\n",
    "                total_loss += self.calc_cost(x_i, y_i)\n",
    "                avg_grad = avg_grad + self.single_grad(x_i, y_i)\n",
    "\n",
    "            #print(f\"Loss at epoch {e} : {total_loss / n}\")\n",
    "            #print(f\"The gradient is : {avg_grad}\")\n",
    "            #print(f\"Gradients : {avg_grad}\")\n",
    "            #avg_grad = avg_grad / n\n",
    "\n",
    "            if self.momentum:\n",
    "                self.calc_velocity(avg_grad)\n",
    "                self.update_weights()\n",
    "            else:\n",
    "                self.update_weights(avg_grad)\n",
    "            print(f\"Weight after update at epoch {e} : {self.w}\")\n",
    "            #print(\"\\n\")\n",
    "        print(\"\\n\")\n",
    "        #Comparing with the efficient version \n",
    "        self.w = np.zeros(6,)\n",
    "\n",
    "\n",
    "        for e in range(self.epochs):\n",
    "            \n",
    "            total_cost = self.full_cost() / n\n",
    "            #print(f\"Loss at epoch {e} : {total_cost}\")\n",
    "            grad = self.full_grad()\n",
    "            #print(f\"The gradient : {grad}\")\n",
    "            self.update_weights(grad)\n",
    "            print(f\"Weight after update at epoch {e} : {self.w}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on a data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5526315789473685\n"
     ]
    }
   ],
   "source": [
    "lr= LogisticRegression(fit_intercept=False)\n",
    "lr = lr.fit(X_train, y_train)\n",
    "preds = lr.predict(X_test)\n",
    "acc_score = accuracy_score(y_test, preds)\n",
    "print(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42574454, -0.37940255]])"
      ]
     },
     "execution_count": 940,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.42430986])"
      ]
     },
     "execution_count": 935,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogReg(y=y_train, X=X_train, eta=0.001, epochs=10000, irls=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss before training : 0.92152379989836\n",
      "The loss after training : 0.67381903941546\n"
     ]
    }
   ],
   "source": [
    "logreg.train_gd_eff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.w = np.array([ 0.42574454, -0.37940255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.42574454, -0.37940255])"
      ]
     },
     "execution_count": 942,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = logreg.full_prob(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.round(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "       1., 1., 1., 1.])"
      ]
     },
     "execution_count": 945,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5526315789473685"
      ]
     },
     "execution_count": 947,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training via Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss before training : 1.7530780370560124\n",
      "The loss after training : 0.672288310185557\n",
      "Done with fold 1\n",
      "The loss before training : 4.351926330287029\n",
      "The loss after training : 0.6907893470225347\n",
      "Done with fold 2\n",
      "The loss before training : 1.5740489414933119\n",
      "The loss after training : 0.6800117669321131\n",
      "Done with fold 3\n"
     ]
    }
   ],
   "source": [
    "num_samples = len(X)\n",
    "indices = np.arange(0, num_samples)\n",
    "np.random.shuffle(indices)\n",
    "k = 3\n",
    "num_val_samples = num_samples // k\n",
    "val_scores = []\n",
    "\n",
    "for fold in range(0,k):\n",
    "    val_set_ix = indices[num_val_samples * fold: num_val_samples * (fold+1)]\n",
    "    train_set_ix = np.concatenate([indices[: num_val_samples * fold],\n",
    "                                   indices[num_val_samples * (fold+1): ]])\n",
    "\n",
    "    X_val = X[val_set_ix]\n",
    "    y_val = y[val_set_ix]\n",
    "\n",
    "    X_train = X[train_set_ix]\n",
    "    y_train = y[train_set_ix]\n",
    "\n",
    "    logreg = LogReg(y=y_train, X=X_train, eta=0.0001, epochs=1000)\n",
    "    logreg.train_gd_eff()\n",
    "\n",
    "    preds = logreg.full_prob(X_val)\n",
    "    preds = np.round(preds)\n",
    "    acc_score = accuracy_score(y_val, preds)\n",
    "\n",
    "    val_scores.append(acc_score)\n",
    "\n",
    "    print(f\"Done with fold {fold+1}\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5238095238095238"
      ]
     },
     "execution_count": 784,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(val_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "\n",
    "    inputs = keras.Input(shape=(2,))\n",
    "    x = keras.layers.Dense(units=256, activation=\"relu\")(inputs)\n",
    "    x = keras.layers.Dense(units=256, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = keras.Model(inputs, output)\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=2e-3), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "22/22 [==============================] - 1s 7ms/step - loss: 0.7976 - accuracy: 0.4659\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.6946 - accuracy: 0.4886\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6739 - accuracy: 0.5568\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.7057 - accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7620 - accuracy: 0.4205\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.7162 - accuracy: 0.5114\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6866 - accuracy: 0.5455\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6701 - accuracy: 0.5455\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6635 - accuracy: 0.6250\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6592 - accuracy: 0.6136\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6701 - accuracy: 0.6023\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6603 - accuracy: 0.6250\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6599 - accuracy: 0.6136\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6571 - accuracy: 0.6477\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6514 - accuracy: 0.6023\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6792 - accuracy: 0.5568\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6660 - accuracy: 0.5682\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6665 - accuracy: 0.6250\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6565 - accuracy: 0.6250\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6620 - accuracy: 0.5795\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6465 - accuracy: 0.6705\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6773 - accuracy: 0.5909\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6571 - accuracy: 0.5795\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6495 - accuracy: 0.6250\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6504 - accuracy: 0.6364\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6466 - accuracy: 0.6477\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6523 - accuracy: 0.6023\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6421 - accuracy: 0.6477\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.6357 - accuracy: 0.6705\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6376 - accuracy: 0.6591\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6334 - accuracy: 0.6477\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6505 - accuracy: 0.6591\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6650 - accuracy: 0.6023\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6373 - accuracy: 0.6591\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.6407 - accuracy: 0.6250\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6344 - accuracy: 0.6591\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6509 - accuracy: 0.6591\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6311 - accuracy: 0.6705\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6311 - accuracy: 0.6591\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6473 - accuracy: 0.6364\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6306 - accuracy: 0.6136\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6170 - accuracy: 0.6818\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6206 - accuracy: 0.6818\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6385 - accuracy: 0.6364\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6290 - accuracy: 0.6705\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6220 - accuracy: 0.6250\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6037 - accuracy: 0.6591\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6339 - accuracy: 0.5795\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6166 - accuracy: 0.6364\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.6284 - accuracy: 0.6818\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5977 - accuracy: 0.6705\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6043 - accuracy: 0.6705\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6240 - accuracy: 0.6250\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5973 - accuracy: 0.7045\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6103 - accuracy: 0.6250\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6174 - accuracy: 0.6932\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6131 - accuracy: 0.6705\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.6080 - accuracy: 0.6932\n",
      "Epoch 59/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.6073 - accuracy: 0.6591\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5778 - accuracy: 0.6932\n",
      "Epoch 61/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5751 - accuracy: 0.7273\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5938 - accuracy: 0.6818\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5716 - accuracy: 0.6818\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5856 - accuracy: 0.6705\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5923 - accuracy: 0.6591\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5789 - accuracy: 0.6705\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5808 - accuracy: 0.6705\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5657 - accuracy: 0.7159\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6728 - accuracy: 0.5795\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6120 - accuracy: 0.6591\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6031 - accuracy: 0.6932\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5935 - accuracy: 0.7045\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6063 - accuracy: 0.6705\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6045 - accuracy: 0.6477\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5784 - accuracy: 0.7045\n",
      "Epoch 76/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5721 - accuracy: 0.6818\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5782 - accuracy: 0.6932\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5823 - accuracy: 0.6932\n",
      "Epoch 79/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5813 - accuracy: 0.6818\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5689 - accuracy: 0.6818\n",
      "Epoch 81/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5564 - accuracy: 0.6818\n",
      "Epoch 82/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5801 - accuracy: 0.6591\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5733 - accuracy: 0.6818\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5655 - accuracy: 0.6932\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5519 - accuracy: 0.6591\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5688 - accuracy: 0.6477\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5661 - accuracy: 0.6477\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5596 - accuracy: 0.7045\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5789 - accuracy: 0.7273\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5685 - accuracy: 0.6932\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.5684 - accuracy: 0.6818\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5625 - accuracy: 0.7159\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6188 - accuracy: 0.6364\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.6180 - accuracy: 0.6591\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5973 - accuracy: 0.6932\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5927 - accuracy: 0.6591\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5860 - accuracy: 0.7045\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5777 - accuracy: 0.6705\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5850 - accuracy: 0.6818\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.5750 - accuracy: 0.6705\n"
     ]
    }
   ],
   "source": [
    "history = nn.fit(x=X_train, y=y_train, epochs=100, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = nn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.round(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5526315789473685"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_score = accuracy_score(y_test, preds)\n",
    "acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
